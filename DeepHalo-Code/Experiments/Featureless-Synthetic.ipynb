{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nGkzF_PclmW5"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)\n",
    "from Model.Featureless import MainNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaw_O_OmRPFA",
    "outputId": "43e5b5c8-ba06-4b80-f5e3-dc3eba2256ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "    print(f\"Use GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") \n",
    "    print(\"Use CPU\")\n",
    "\n",
    "\n",
    "def load_or_calc_save_freq_data(file_path, X, Y):\n",
    "  if os.path.exists(file_path):\n",
    "    data = torch.tensor(pd.read_csv(file_path).values, dtype=torch.float)\n",
    "  else:\n",
    "    data = calc_freq(X, Y)\n",
    "    pd.DataFrame(data.cpu().numpy()).to_csv(file_path, index=False)  # 保存到文件\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pCGwwT7DlnsW"
   },
   "outputs": [],
   "source": [
    "def calc_freq(X, Y):\n",
    "    X = X.float()\n",
    "    Y = Y.float()\n",
    "    unique_X, inverse_indices = torch.unique(X, dim=0, return_inverse=True)\n",
    "    new_Y = torch.zeros_like(Y)\n",
    "    for k in range(unique_X.shape[0]):\n",
    "        mask = (inverse_indices == k)\n",
    "        avg_y = torch.mean(Y[mask], dim=0)\n",
    "        new_Y[mask] = avg_y\n",
    "    return new_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "drgmxI0YQ_Z3"
   },
   "outputs": [],
   "source": [
    "def train_deephalo_synthetic(loss_name, num_epochs, depth, reswidth, batch_size, input_dim, lr, patience):\n",
    "    main_network = MainNetwork(input_dim, depth, reswidth, ['qua']*(depth-1))\n",
    "    main_network.to(device)\n",
    "\n",
    "    start_epoch = 0\n",
    "    loss_record = {'insample_rmse_list': [], 'insample_rmse_freq_list': [], 'outsample_rmse_list': [], 'outsample_rmse_freq_list': [], 'insample_step_rmse_list': []}\n",
    "    train_file_path = os.path.join(project_root, 'Data', 'Synthetic_20-15-80_Train.csv')\n",
    "    test_file_path = os.path.join(project_root, 'Data', 'Synthetic_20-15-20_Test.csv')\n",
    "\n",
    "    df_train = pd.read_csv(train_file_path)\n",
    "    df_test = pd.read_csv(test_file_path)\n",
    "\n",
    "    X_columns = [col for col in df_train.columns if col.startswith('X')]\n",
    "    Y_columns = [col for col in df_train.columns if col.startswith('Y')]\n",
    "\n",
    "    X_train = torch.tensor(df_train[X_columns].values, dtype=torch.float)\n",
    "    Y_train = torch.tensor(df_train[Y_columns].values, dtype=torch.float)\n",
    "\n",
    "    X_test = torch.tensor(df_test[X_columns].values, dtype=torch.float)\n",
    "    Y_test = torch.tensor(df_test[Y_columns].values, dtype=torch.float)\n",
    "\n",
    "    train_freq_file_path = os.path.join(project_root, 'Data', 'Synthetic_20-15-80_Train_Freq.csv')\n",
    "    test_freq_file_path = os.path.join(project_root, 'Data', 'Synthetic_20-15-20_Test_Freq.csv')\n",
    "\n",
    "    Y_train_freq = load_or_calc_save_freq_data(train_freq_file_path, X_train, Y_train)\n",
    "    Y_test_freq = load_or_calc_save_freq_data(test_freq_file_path, X_test, Y_test)\n",
    "\n",
    "    X_train_eval = X_train.to(device)\n",
    "    Y_train_eval = Y_train.to(device)\n",
    "    Y_train_freq_eval = Y_train_freq.to(device)\n",
    "\n",
    "    X_test_eval = X_test.to(device)\n",
    "    Y_test_eval = Y_test.to(device)\n",
    "    Y_test_freq_eval = Y_test_freq.to(device)\n",
    "\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = Adam(main_network.parameters(), lr=lr)\n",
    "\n",
    "    in_sample_loss = None\n",
    "    in_sample_freq_loss = None\n",
    "    out_sample_loss = None\n",
    "    out_sample_freq_loss = None\n",
    "\n",
    "    in_sample_freq_loss_list = []\n",
    "    out_sample_freq_loss_list = []\n",
    "\n",
    "    in_sample_loss_list = []\n",
    "    out_sample_loss_list = []\n",
    "\n",
    "    step_loss_list = []\n",
    "\n",
    "    if loss_name == 'NLL':\n",
    "        L = log_likelihood\n",
    "    else:\n",
    "        L = nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    logs = f\"Resuming training for model with Depth {depth} and Width {reswidth} from epoch {start_epoch}.\" + '\\n'\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        for data, target in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _ = main_network(data)\n",
    "            loss = L(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                step_loss_list.append(loss.item() ** 0.5)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            in_sample_output, _ = main_network(X_train_eval)\n",
    "            out_sample_output, _ = main_network(X_test_eval)\n",
    "\n",
    "            in_sample_freq_loss = L(in_sample_output, Y_train_freq_eval).item() ** 0.5\n",
    "            in_sample_loss = L(in_sample_output, Y_train_eval).item() ** 0.5\n",
    "\n",
    "            out_sample_freq_loss = L(out_sample_output, Y_test_freq_eval).item() ** 0.5\n",
    "            out_sample_loss = L(out_sample_output, Y_test_eval).item() ** 0.5\n",
    "\n",
    "            in_sample_freq_loss_list.append(in_sample_freq_loss)\n",
    "            in_sample_loss_list.append(in_sample_loss)\n",
    "            out_sample_freq_loss_list.append(out_sample_freq_loss)\n",
    "            out_sample_loss_list.append(out_sample_loss)\n",
    "\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            logs += f'Epoch [{epoch + 1}/{num_epochs}], Original Loss: {in_sample_loss:.4f},' \\\n",
    "                    f' Frequency Loss: {in_sample_freq_loss:.8f}\\n'\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print(logs)\n",
    "        plt.plot(in_sample_loss_list, label='In-Sample Loss')\n",
    "        plt.title(\"Loss Curve\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "        \n",
    "        if patience:\n",
    "            if in_sample_loss < best_loss - 1e-5:\n",
    "                best_loss = in_sample_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "    \n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "    model_info = {\n",
    "    'model_para': main_network.state_dict(),\n",
    "\n",
    "    'model_hyperpara': {'depth': depth,\n",
    "                        'resnet_width': reswidth,\n",
    "                        'epoch': num_epochs,\n",
    "                        'batch_size': batch_size,\n",
    "                        'lr': lr,\n",
    "                        'loss_name': loss_name,\n",
    "                        'para_num': num_params},\n",
    "\n",
    "    'result': {'insample_rmse': in_sample_loss,\n",
    "               'insample_rmse_freq': in_sample_freq_loss,\n",
    "               'outsample_rmse': out_sample_loss,\n",
    "               'outsample_rmse_freq': out_sample_freq_loss},\n",
    "\n",
    "    'loss_record': {'insample_rmse_list': in_sample_loss_list,\n",
    "                    'insample_rmse_freq_list': in_sample_freq_loss_list,\n",
    "                    'outsample_rmse_list': out_sample_loss_list,\n",
    "                    'outsample_rmse_freq_list': out_sample_freq_loss_list,\n",
    "                    'insample_step_rmse_list': step_loss_list}}\n",
    "    loss_record =  {'insample_rmse_list': in_sample_loss_list,\n",
    "                    'insample_rmse_freq_list': in_sample_freq_loss_list,\n",
    "                    'outsample_rmse_list': out_sample_loss_list,\n",
    "                    'outsample_rmse_freq_list': out_sample_freq_loss_list,\n",
    "                    'insample_step_rmse_list': step_loss_list}\n",
    "\n",
    "    file_path = os.path.join(project_root, 'Model', 'Synthetic')\n",
    "    torch.save(model_info, file_path + 'Depth-' + str(depth) + '-Width-'+ str(reswidth) + 'Model-syn-20-15-80-20.pth')\n",
    "\n",
    "    print(model_info['model_hyperpara'])\n",
    "    print(model_info['result'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "YMXYNyEs01TM"
   },
   "outputs": [],
   "source": [
    "def param_count(depth, width):\n",
    "    return (depth - 1) * (width**2 + width) + 42 * width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEf4oLX62L6h",
    "outputId": "5c15d563-85a4-4d57-da5e-3b9d3913f332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth 3 width 489 params 499758\n",
      "depth 4 width 401 params 500448\n",
      "depth 5 width 348 params 500424\n",
      "depth 6 width 312 params 501384\n",
      "depth 7 width 285 params 501030\n"
     ]
    }
   ],
   "source": [
    "def count_params(depth, width):\n",
    "    return (depth - 1) * (width ** 2 + width) + 42 * width\n",
    "\n",
    "budget = 500000\n",
    "tolerance = 0.003  # ±2%\n",
    "min_params = int(budget * (1 - tolerance))\n",
    "max_params = int(budget * (1 + tolerance))\n",
    "\n",
    "candidates = []\n",
    "for depth in range(3, 8):  # must have at least input/output layers\n",
    "    for width in range(100, 1000):\n",
    "        p = count_params(depth, width)\n",
    "        if min_params <= p <= max_params:\n",
    "            candidates.append((depth, width, p))\n",
    "\n",
    "# Sort by width or depth if needed\n",
    "candidates.sort(key=lambda x: x[0])  # sort by depth\n",
    "\n",
    "\n",
    "for d, w, p in candidates[:10]:\n",
    "    print('depth', d, 'width', w, 'params', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891
    },
    "id": "Xzanz5UK6rur",
    "outputId": "e1c345ff-1074-43cd-ca0a-4be0d419fa46"
   },
   "outputs": [],
   "source": [
    "Loss_Name = 'MSE'\n",
    "epochs = 5\n",
    "dim = 20\n",
    "batch_size = 1024\n",
    "num_workers = 0\n",
    "learning_rate = 0.0001\n",
    "patience = None\n",
    "experiments = {\n",
    "    # '10k': [(3, 60), (4, 51), (5, 45), (6, 40), (7, 37)],\n",
    "    # '30k': [(4, 60), (5, 52), (6, 48), (7, 44)],\n",
    "    # '200k': [(3, 306), (4, 251), (5, 218), (6, 195), (7, 179)],\n",
    "    '500k': [(7, 285), (6, 312), (5, 348), (4, 401), (3, 489)],\n",
    "}\n",
    "\n",
    "for budget, configs in experiments.items():\n",
    "    print(f'=== Testing Budget Group: {budget} ===')\n",
    "    for depth, width in configs:\n",
    "        num_params = param_count(depth, width)\n",
    "        print(f'Depth: {depth}, Width: {width} → Parameter Num: {num_params}')\n",
    "        train_deephalo_synthetic(Loss_Name, epochs, depth, width, batch_size, dim, learning_rate, patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
