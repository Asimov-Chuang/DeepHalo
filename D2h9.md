#### 1. Clarifying the relationship between DeepHalo and Transformer architectures

This is a very insightful observation—indeed, the connection to Transformer-style architectures is deeply embedded in the development of our approach, and we are grateful for the opportunity to elaborate.

Let us start from the **featureless setting**. Here, modeling higher-order context effects reduces to **polynomial regression over indicator functions** (i.e., whether an item is present). DeepHalo provides a **structured and recursive mechanism** to capture subset-level interactions in this space—conceptually similar to accumulating interactions based on indicators.

In the **feature-rich setting**, the challenge becomes: how do we encode interactions among a subset of items using their feature vectors, **without incorporating signals from outside the subset**? This naturally evokes the idea of **attention**, where a subset's effect is modeled as a function of feature vectors aggregated across the subset. If attention weights are uniform, the update simplifies to a **linear transformation followed by a nonlinearity**, which closely resembles our formulation.

However, there are several **critical differences between DeepHalo and Transformers**, both structurally and in modeling implications:

- **Effect order control**:  
  Transformers (e.g., in TCNet) implicitly allow interactions across all items, resulting in **full-order interactions** (order $|S| - 1$).  
  In contrast, **DeepHalo provides explicit layer-wise control**, where each layer adds exactly one level of interaction. This control is essential for **interpretability, regularization, and tractability**.

- **Structural simplification**:  
  DeepHalo **eliminates the need for query/key/value projections and softmax attention**, replacing them with **fixed linear transformations and polynomial activations**.  
  This results in **lower computational cost**—linear in $|S|$, rather than quadratic—as well as a simpler and more transparent architecture.

- **Interpretability and decomposition**:  
  DeepHalo is explicitly designed for **polynomial decomposition**: each layer contributes to a specific interaction order (first, second, ..., $L$-th), enabling **structured and interpretable utility functions**.  
  Standard Transformers entangle interactions across layers and attention heads, making **such decomposition intractable**.

In short, while DeepHalo draws conceptual inspiration from Transformer-style aggregation, it **diverges in architectural design to achieve interpretable, order-controllable, and computationally efficient modeling** of contextual choice effects.

We thank the reviewer again for raising this connection, and will add a discussion in the final version to highlight these points explicitly.

