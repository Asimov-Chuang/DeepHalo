We thank Reviewer D2h9 for their positive recognition of our work. In the revision, we will include a model architecture diagram and correct the noted typos. Below, we provide point-by-point responses to the reviewer’s comments.


### Responses to Weakness

> Q: In experiments, I think it is essential to provide the number of parameters of each benchmark model to make sure the better performance of DeepHalo is from the design instead of the size.

A: We thank the reviewer for raising the question regarding the model scale. To address this, we computed the number of trainable parameters under the **Expedia experiment setting**, where the input item feature dimension is set to 10 across all models. The results are summarized below:

| Model      | Number of Parameters |
|------------|----------------------|
| TCNet      | 118,209              |
| **DeepHalo**   | **53,833**               |
| RUMNet     | 53,900               |
| TasteNet   | 35,851               |
| DLCL       | 210                  |
| FATE       | 44,363               |
| MLP        | 42,601               |
| ResLogit   | 2,720                |
| MNL        | 10                   |

> Q: While many benchmark models are compared in datasets with features (Table 2), while in featureless datasets (Table 1), only a few models are tested. However, I think the models like FateNet, TCNet can also be applied to featureless cases.

A: We thank the reviewer for their suggestion. In the **featureless setting**, we have added results for three representative baselines: **FATE**, **TCNet**, and **Mixed MNL**. To ensure fairness and focus on architectural differences, we controlled the parameter size of all neural network–based models to be **approximately 1K parameters**. The results are shown below:

| Model         | Hotel (Train/Test) | SFOshop (Train/Test) | SFOwork (Train/Test) |
|---------------|--------------------|------------------------|-----------------------|
| MNL           | 0.7743 / 0.7743    | 1.7281 / 1.7262        | 0.9423 / 0.9482       |
| MLP           | 0.7569 / 0.7523    | 1.5556 / 1.5523        | 0.8074 / 0.8120       |
| CMNL          | 0.7566 / 0.7561    | 1.5676 / 1.5686        | 0.8116 / 0.8164       |
| Mixed MNL     | 0.7635 / 0.7613    | 1.5599 / 1.5577        | 0.8092 / 0.8153       |
| FateNet       | 0.7575 / 0.7568    | 1.5726 / 1.5765        | 0.8133 / 0.8167       |
| TCNet         | **0.7467** / 0.7670| 1.5694 / 1.5742        | 0.8114 / 0.8145       |
| **DeepHalo** (Ours) | 0.7479 / **0.7483** | **1.5385** / **1.5263** | **0.8040** / **0.8066** |

### Responses to Questions

> Q: The formulation of higher-order interactions in Equations (4) and (5) appears conceptually similar to the transformer architecture, with the key difference being that the attention mechanism is replaced by a linear transformation followed by an activation function (just like assigning equal attention across elements). Could you elaborate on the differences between your approach and transformers, both in terms of formulation and implications for model expressiveness or computational efficiency?

A: This is a highly insightful observation. Indeed, the connection to Transformer-style architectures is deeply embedded in the design of our approach, and we appreciate the opportunity to elaborate on this.

In summary, there are two differences between DeepHalo and Transformers, both structurally and in modeling implications:

- **Effect order control**:  
  Transformers (e.g., in TCNet) implicitly allow interactions across all items, inevitably resulting in **full-order interactions** (order $|S| - 1$). In contrast, DeepHalo provides explicit layer-wise control, where each layer adds exactly one level of interaction (or $\times 2$ with quadratic activation). This control is essential for interpretability, regularization, and tractability.

- **Structural simplification**:  
  DeepHalo eliminates the need for query/key/value projections and softmax attention, replacing them with fixed linear transformations and polynomial activations. This results in lower computational cost—linear in $|S|$, rather than quadratic—as well as a simpler and more transparent architecture.

Below, we briefly explain the motivation behind our model design to provide a more intuitive understanding of our model architecture.

We begin with the featureless setting, where modeling higher-order context effects reduces to polynomial regression over indicator functions that encode item presence. DeepHalo introduces a structured and recursive mechanism to capture subset-level interactions in this setting, conceptually analogous to the accumulation of interactions based on binary inclusion indicators.

In the feature-rich setting, the core challenge is to encode interactions among a subset of items using their feature representations, while strictly excluding information from outside the subset. This naturally recalls the concept of attention scores, typically computed from item feature pairs. However, when these scores are normalized via a softmax function, the resulting attention weights entangle all item features—thus violating the independence required for controlled halo modeling.

For instance, when evaluating the influence of subset $\{A, B\}$ on the utility of item $C$ within the offer set $\{A, B, C, D\}$, the effect should depend solely on the features of $A$, $B$, and $C$, and remain invariant to unrelated items such as $D$. 

To address this, we omit softmax normalization, and further observe that the dot-product attention mechanism is not essential for modeling decomposable utility functions. This insight motivates a simplification of the architecture, leading to the formulations presented in Equations (4) and (5).

In short, while DeepHalo draws conceptual inspiration from Transformer-style aggregation, it diverges in architectural design to achieve interpretable, order-controllable, and computationally efficient modeling of contextual choice effects. We thank the reviewer again for raising this connection and will add a discussion in the final version to highlight these points explicitly.



